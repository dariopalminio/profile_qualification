# Desarrollador BigData

## Otros Nombres

Data Engineer (EN), Big Data Engineer (EN), Ingeniero de Datos, Desarrollador Big Data, Engenheiro de dados (PT, BR)

## Descripción

Profesional desarrollador de Big Data, encargado de diseñar, construir y gestionar los datos y la infraestructura necesaria para almacenarlos y procesarlos. Construyen la base tecnológica para que los científicos de datos y analistas puedan realizar sus tareas. Por lo tanto, son los responsables de mantener sistemas de Big Data escalables, con alta disponibilidad y rendimiento, integrando nuevas tecnologías y desarrollando el software necesario. Responsable de definir cómo gestionar, organizar, transformar y almacenar los datos necesarios en la organización de una forma óptima para todos los interesados. 

## Formación principal y académica

Titulación en Ingeniería Informática, Ingeniería de Sistemas, o Licenciados en Matemáticas u otras ingenierías con experiencia significativa en programación. 

## Formación secundaria

Máster o titulación de posgrado en temas relacionados con Data Science, Big Data y/o DevOps, Inteligencia Artificial y Machine Learning; o en su defecto experiencia demostrable en estos temas. 

## Supervisión o liderazgo

No cuenta con personal a su cargo. 

## Conocimientos y competencias

Debe tener los siguientes conocimientos y competencias con la profundidad dependiente del nivel de seniority requerido:

1. Tener conocimientos informáticos. Deseable también matemáticos y estadísticos.  
2. Saber programar en lenguajes de scripting como Perl; o lenguajes como Python , R, Spark o Scala. 
3. Conocimientos en automatización, scripting y en sistemas Unix/Linux. 
4. Conocer sobre Gobierno de datos y seguridad de datos. 
5. Debe conocer el diseño y desarrollo de sistemas de arquitectura de datos. 
6. Debe dominar el procesamiento de datos (Data Base, DM, Data warehouse), que incluye implementación y manipulación de bases de datos relacionales (SQL, PL/SQL) y no relacionales (NoSQL). 
7. La capacidad para implementar tecnologías de gestión de datos, reportería y elaboración de informes. 
8. Comprensión competente de los principios de computación distribuida.
9. Uso de herramientas de BI y visualización de datos. 
10. Conocimientos en infraestructuras Cloud (Azure, AWS y/o Google Cloud Platform). 
11. Conocimientos en Big Data, Machine Learning, Deep Learning, Data wrangling, Data mining, Data Lake, Data Workflow, Procesos ETL, data munging o data tyding. 
12. Conocimiento y uso de herramientas de Big Data y explotación de datos. 
13. Experiencia con la integración de datos de múltiples fuentes de datos.
14. Conocimientos en sistemas de mensajería y programación orientada a eventos.
15. Buen conocimiento de la Arquitectura Lambda, junto con sus ventajas y desventajas.

## Responsabilidades y funciones

1. Construir y mejorar Stack tecnológico para Big Data y apoyo en la planificación de soluciones de Big Data. 
2. Programar aplicaciones distribuidas que manejen un gran volumen de datos con tecnologías/framework como Hadoop o Apache Spark. 
3. Identificación y consultas en fuentes de datos ejecutando operaciones sobre bases de datos con SQL y NoSQL. 
4. Ejecución del flujo de datos. Diseño e implementación de flujos de captura y tratamiento de datos masivos y/o en tiempo real. 
5. Encadenar las rutinas y los procesos a través de redes de pipelines para automatizar su principal tarea: ETL (Extract, Transform, Load). 
6. Garantizar la accesibilidad, precisión y seguridad de los datos. 
7. Apoyar y facilitar el trabajo a analistas y científicos de datos, así como a negocio. 

## Tecnologías

- Base de datos relacional (SQL): Oracle, MariaDB, MySQL, PostgreSQL, etc.
- Base de datos no relacional (NoSQL): MongoDB, Redis, Cassandra, HBase, etc.
- Big Data querying tools: Pig, Hive, Impala, etc.
- Tecnología cloud computing: Google Cloud Platform, AWS, Asure, etc.
- Motores de BigData: Spark (Spark/Scala/HDFS/Hive, Python, SQL, Shell).
- Framework de programación distribuida de BigData: Hadoop (Hadoop, Spark,  HDFS , Map Reduce, Yarn, PIG, HIVE, HBase, Mahout, Spark MLLib, Solar, Lucene, Zookeeper, Oozie ).
- Herramientas de automatización: Ansible, Control M, etc.
- ETL tools: Apache Flume, Apache Sqoop, Apache HBase, Apache Hive, Apache Oozie, Apache Phoenix, Apache Pig, Apache ZooKeeper.
- Event & messaging systems: Kafka, RabbitMQ, etc.
- Big Data Machine Learning toolkits: Mahout, SparkML, H2O, etc.
