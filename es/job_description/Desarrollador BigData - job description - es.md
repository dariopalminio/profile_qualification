# Data Engineer (Desarrollador BigData)

## Otros Nombres

Data Engineer (EN), Big Data Engineer (EN), Ingeniero de Datos, Desarrollador Big Data, Engenheiro de dados (PT, BR)

## Descripción

Profesional Ingeniero de Datos y desarrollador de Big Data, encargado de diseñar, construir, gestionar y mantener los datos y la infraestructura necesaria para almacenarlos y procesarlos, necesaria para gestionar y procesar información de manera eficiente y confiable. Este profesional habilita la recopilación, procesamiento y uso efectivo de los datos. Construyen la base tecnológica para que los científicos de datos y analistas puedan realizar sus tareas. Por lo tanto, son los responsables de mantener sistemas de Datos o Big Data escalables, con alta disponibilidad y rendimiento, integrando nuevas tecnologías y desarrollando el software necesario. Responsable de definir cómo gestionar, organizar, transformar y almacenar los datos necesarios en la organización de una forma óptima para todos los interesados. 

## Formación principal y académica

Titulación en Ingeniería Informática, Ingeniería de Sistemas, o Licenciados en Matemáticas u otras ingenierías con experiencia significativa en programación. 

## Formación secundaria

Máster o titulación de posgrado en temas relacionados con Data Science, Big Data y/o DevOps, Data & Cloud
Engineer, Inteligencia Artificial y Machine Learning; o en su defecto experiencia demostrable en estos temas. Ejemplos de certificaciones: Google Cloud Professional Data Engineer Certification, AWS Certified Data Engineer.

## Supervisión o liderazgo

No cuenta con personal a su cargo. 

## Conocimientos y competencias

Debe tener los siguientes conocimientos y competencias con la profundidad dependiente del nivel de seniority requerido:

### Programación y Desarrollo de Software
- Lenguajes de Programación: Perl; Python , R, Spark o Scala.
- Control de Versiones: Git.
- Desarrollo de APIs: REST, SOAP.
- Arquitectura de Eventos: Conocimientos en sistemas de mensajería, colas, programación orientada a eventos (arquitectura basada en eventos EDA, Pub/Sub).
- Principios de Ingeniería de Software: Diseño modular, pruebas unitarias y de integración.

### Manejo de Datos
- Bases de Datos SQL: PostgreSQL, MySQL, SQL Server.
- Bases de Datos NoSQL: MongoDB, Cassandra, HBase.
- Modelado de Datos: Diseño de esquemas, normalización y denormalización.
- ETL (Extract, Transform, Load): Herramientas como Apache NiFi, Talend, Informatica.
- Big Data y Computación Distribuida

### Plataformas de Big Data: Hadoop, Spark.
- Almacenamiento y Procesamiento Distribuido: HDFS, Amazon S3.
- Procesamiento en Streaming: Kafka, Apache Storm, Apache Flink.

### Infraestructura y Nube
- Servicios de Nube: AWS (EC2, RDS, S3), Azure, Google Cloud (GCP).
- Orquestación de Contenedores: Docker, Kubernetes.
- Automatización de Infraestructura: Terraform, Ansible.

### Seguridad y Gobernanza de Datos
- Seguridad de Datos: Encriptación, autenticación, autorización.
- Gobernanza de Datos: Catalogación de datos, gestión de metadatos, calidad de datos.

### Analítica y Aprendizaje Automático
- Fundamentos de Machine Learning: Uso de librerías como scikit-learn, TensorFlow, PyTorch.
- Optimización y Escalamiento de Modelos: Ajuste de hiperparámetros, despliegue de modelos en producción.

### Orquestación de Flujos de Trabajo
- Conocimientos y practica en orquestación de flujos de datos.
  
### Visualización
- Herramientas de Visualización de Datos: Tableau, Power BI, Dash de Plotly.

### Trabajo en Equipo
- Sentido de responsabilidad, compromiso y capacidad de trabajo en equipo. 


## Responsabilidades y funciones
Debe aplicar los conocimientos y competencias descritos anteriormente. A continuación algunas responsabilidades y funciones:

1. Construir y mejorar Stack tecnológico para Big Data y apoyo en la planificación de soluciones de Big Data. 
2. Programar aplicaciones distribuidas que manejen un gran volumen de datos con tecnologías/framework como Hadoop o Apache Spark. 
3. Identificación y consultas en fuentes de datos ejecutando operaciones sobre bases de datos con SQL y NoSQL. 
4. Ejecución del flujo de datos. Diseño e implementación de flujos de captura y tratamiento de datos masivos y/o en tiempo real. 
5. Encadenar las rutinas y los procesos a través de redes de pipelines para automatizar su principal tarea: ETL (Extract, Transform, Load). Automatizar tareas y pipelines que almacenan y procesan datos en formatos y tecnologías apropiadas
6. Garantizar la accesibilidad, precisión y seguridad de los datos. 
7. Apoyar y facilitar el trabajo a analistas y científicos de datos, así como a negocio. 
8. Estimar esfuerzo de desarrollo de componentes y tareas además de colaborar con la planificación de trabajo.
9. En el trabajo orientado a equipo, aunque trabajará la mayor parte del tiempo en su campo de experticia, debe desenvolverse en otros temas, tareas y actividades en función de colaborar con el equipo, apoyar en diferentes tipos de actividades para sumar capacidad, cumplir con las entregas del equipo, evitar los cuellos de botella y colaborar en solucionar problemas del equipo.
10. Solución de Problemas y Mejoras a procesos
11. Documentar Metadatos.

## Tecnologías

La tecnología dependerá de los requisitos del puesto. La tecnología general relacionada es:
- Operating System: Unix, Linux,  Windows, Mac OS.
- SQL Data Base: Oracle, MariaDB, MySQL, PostgreSQL, etc.
- NoSQL Data Base: MongoDB, Redis, Cassandra, HBase, etc.
- Big Data querying tools: Pig, Hive, Impala, etc.
- Cloud Computing: Google Cloud Platform, AWS, Asure, etc.
- BigData engine: Spark (Spark/Scala/HDFS/Hive, Python, SQL, Shell).
- BigData distributed framework: Hadoop (Hadoop, Spark,  HDFS , Map Reduce, Yarn, PIG, HIVE, HBase, Mahout, Spark MLLib, Solar, Lucene, Zookeeper, Oozie ).
- Automation tools: Ansible, Control M, etc.
- ETL tools: Apache Flume, Apache Sqoop, Apache HBase, Apache Hive, Apache Oozie, Apache Phoenix, Apache Pig, Apache ZooKeeper.
- Event & Message Brokers: Kafka, RabbitMQ, etc.
- Big Data Machine Learning toolkits: Mahout, SparkML, H2O, etc.
- Orquestación de flujos de datos con herramientas como: Airflow, Apache NiFi, Luigi, Prefect, Argo Workflows.
- Orquestadores Cloud: Google Cloud Composer, Azure Data Factory o AWS Step Functions.
