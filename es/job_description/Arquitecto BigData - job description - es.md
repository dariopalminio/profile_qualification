# Arquitecto BigData

## Otros Nombres

Big Data Architect (EN), Data Architect (EN), Arquitecto de Datos, Arquitecto Big Data, Arquiteto de Dados (PT, BR)

## Descripción

Experto digital en arquitectura de Datos y/o Big Data, que se encarga de plantear la estrategia de datos de la empresa, área u organización, influyendo sus estándares de calidad, el tratamiento del flujo de datos dentro de la organización y la seguridad de los mismos. Responsable del diseño, la estructura y el mantenimiento de grandes volúmenes de datos y su infraestructura de procesamiento, garantizando la precisión, accesibilidad de los datos relevantes, soluciones robustas y escalables que adhieran a estándares y guías tecnológicas en el ámbito de los datos y analítica avanzada. 

## Formación principal y académica

Profesional del área TI, con título en carreras como: Ingeniero de Sistemas, Ingeniería Informática, Licenciatura en Sistemas o Computación,etc. 

## Formación secundaria

Master en Big Data, Postgrado en Inteligencia Artificial y/o Machine Learning.

## Supervisión o liderazgo

Puede o no contar con personal a su cargo. 

## Conocimientos y competencias

Debe tener los siguientes conocimientos y competencias con la profundidad dependiente del nivel de seniority requerido:

1. **Informática**:Tener conocimientos informáticos, matemáticos y estadísticos.  
2. **Programación**:Saber programar en lenguajes como: R, Python, C, Java y Perl. 
3. **Gobierno de datos**: Conocer sobre Gobierno de datos (DAMA, DMBOK) y seguridad de datos.  
4. **Arquitectura de Datos**: Debe conocer el diseño y desarrollo de sistemas de arquitectura de datos: arquitectura distribuida, patrones de arquitectura y principios de arquitectura Big Data. 
5. **Patrones de Datos**: Conocimiento en mecanismos y patrones de Big Data: Data Source Patterns, Storage Patterns, Data Processing Patterns, Data Transfer and Transformation Patterns, Data Maintenance Patterns, Compound Patterns, etc.
6. **Modelado de Datos**: Debe dominar el modelado y diseño de datos (Data Base, DM, Data warehouse), que incluye el diseño y estructura de bases de datos relacionales (SQL, PL/SQL) y no relacionales (NoSQL). 
7. El conocimiento sobre modelado predictivo, sistemas ERP y CRM, así como el análisis de texto. También su formación en aprendizaje automático. 
8. **Reportería**: La capacidad para implementar tecnologías de presentación de datos, reportería y elaboración de informes. 
9. Saber sobre Machine Learning, Deep Learning, Data wrangling, Data mining, Data Lake, Data Workflow, Procesos ETL, data mesh, data munging o data tyding. 
10. **Big Data y explotación**: Conocimiento y uso de herramientas de Big Data y explotación de datos.
11. **Cloud Computing**: Conocimientos en Cloud Computing, 'Cloud-based Big Data Processing' y arquitectura Cloud en general (Cloud Computing Design Patterns & Mechanisms).

## Responsabilidades y funciones

1. Ser referente de la estrategia de datos y analítica avanzada de la empresa.
2. Desarrollo de estrategia de datos. 
3. Definir o colaborar en  la elección de tecnologías o Stack tecnológico para Arquitectura de Datos o Big Data. 
4. Identificación de fuentes de datos. 
5. Diseño y gestión de la arquitectura de datos. 
6. Planificación de soluciones de Big Data. 
7. Gestión del flujo de datos. 
8. Garantizar la accesibilidad, precisión y seguridad de los datos. 
9. Realización de evaluación y auditorías de datos.
10. Analizar el impacto comercial que ciertas opciones técnicas pueden tener en los procesos comerciales  o de negocio. 
11. Proporcionar especificaciones según las cuales se define, gestiona y entrega la solución. 
12. Investigar continuamente tecnologías emergentes y propone cambios a la arquitectura existente para reducir deuda técnica y mejorar la solución. 
13. Asegurar el cumplimiento de requerimientos no funcionales y calidad del software: medir la performance de la aplicación y conducir pruebas en relación a la performance, seguridad, etc.
14. Definir y liderar los estándares, mejores prácticas y patrones de diseño para la apropiada implementación de productos de datos.
15. Liderazgo técnico: Supervisar, liderar y/o guiar al o los equipos de desarrollo. Realizar coaching y mentoring sobre problemas técnicos, ayudando a la evolución profesional del equipo de programadores.
16. Estimar esfuerzo de desarrollo de componentes y tareas además de colaborar con la planificación de trabajo. 
17. En el trabajo orientado a equipo, aunque trabajará la mayor parte del tiempo en su campo de experticia, debe desenvolverse en otros temas, tareas y actividades en función de colaborar con el equipo, apoyar en diferentes tipos de actividades para sumar capacidad, cumplir con las entregas del equipo, evitar los cuellos de botella y colaborar en solucionar problemas del equipo.
18. Acompañar en el diseño e implementación de productos de datos en la organización.

## Tecnologías

La tecnología dependerá de los requisitos del puesto. La tecnología general relacionada es:
- Diagramming: Enterprise Architect, Draw.io (diagrams.net), VisualParadigm, Lucidchart, Visio.
- Ingest & Collect: Logstash, Log4J, Fluentd, IOT, GCP (Cloud SDK, Cloud Dataproc), etc.
- Herramientas de automatización: Ansible, Control M, etc.
- Storage, Base de datos relacional (SQL): Oracle, MariaDB, MySQL, PostgreSQL, etc.
- Storage, Base de datos no relacional (NoSQL): MongoDB, Redis, DynamoDb AWS, Cassandra, HBase, etc.
- Storage, File Storage: HDFS, Amazon (S3, Glacier), GCP (Cloud Storage, Cloud SQL, BigQuery), etc.
- Store - Stream Storage, Event & messaging systems: Amazon (Kinesis, DynamoDB Stream, SQS, SNS), Apache Kafka Stream, RabbitMQ, etc.
- Process & Analyze, Big Data querying tools: GCP (Cloud Dataproc, BigQuery), Pig, Apache Hive, Impala, Apache Solr, Apache Spooq, Apache Zeppelin, Apache HBase, etc.
- Process & Analyze, Big Data Machine Learning toolkits: Mahout, SparkML, H2O, etc.
- Motores de BigData: Spark (Spark/Scala/HDFS/Hive, Python, SQL, Shell).
- Framework de programación distribuida de BigData: Hadoop (Hadoop, Spark, HDFS , Map Reduce, Yarn, PIG, HIVE, HBase, Mahout, Spark MLLib, Solar, Lucene, Zookeeper, Oozie ).
- ETL tools: Apache Flume, Apache Sqoop, Apache HBase, Apache Hive, Apache Oozie, Apache Phoenix, Apache Pig, Apache ZooKeeper, Cloud ETL (Elastic MapReduce AWS, AWS Glue, Azure Data Factory, Google Cloud Dataflow).
- Cloud Computing Provider: Google Cloud Platform, AWS, Asure, IBM Cloud Pak, etc.
- Consumer tools: Amazon (QuickSight), Tableau, Looker, TIBC Jaspersoft, Microstrategy, Kibana, Flot, Apache Zeppelin, GCP (Data Studio), IPython, Power BI, Power Apps, IBM Cognos, etc.
- Security: Apache Knox, Apache Ranger, etc.

  

