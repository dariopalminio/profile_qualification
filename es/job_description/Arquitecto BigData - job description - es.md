# Arquitecto BigData

## Otros Nombres

Data Architect (EN), Arquitecto de Datos, Arquitecto Big Data, Arquiteto de Dados (PT, BR)

## Descripción

Experto digital que se encarga de plantear la estrategia de datos de la empresa, área u organización, influyendo sus estándares de calidad, el tratamiento del flujo de datos dentro de la organización y la seguridad de los mismos. Responsable del diseño, la estructura y el mantenimiento de grandes volúmenes de datos y su infraestructura de procesamiento, garantizando la precisión y accesibilidad de los datos relevantes. 

## Formación principal y académica

Profesional del área TI, con título en carreras como: Ingeniero de Sistemas, Ingeniería Informática, Licenciatura en Sistemas o Computación,etc. 

## Formación secundaria

Master en Big Data, Postgrado en Inteligencia Artificial y/o Machine Learning.

## Supervisión o liderazgo

No cuenta con personal a su cargo. 

## Conocimientos y competencias

Debe tener los siguientes conocimientos y competencias con la profundidad dependiente del nivel de seniority requerido:

1. Tener conocimientos informáticos, matemáticos y estadísticos.  
2. Saber programar en lenguajes como: R, Python, C, Java y Perl. 
3. Conocer sobre Gobierno de datos y seguridad de datos. 
4. Debe conocer el diseño y desarrollo de sistemas de arquitectura de datos: Arquitectura distribuida, Patrones de Big Data ('Big Data Patterns and Mechanisms') y principios de Big Data Architecture. 
5. Debe dominar el modelado y diseño de datos (Data Base, DM, Data warehouse), que incluye el diseño y estructura de bases de datos relacionales (SQL, PL/SQL) y no relacionales (NoSQL). 
6. El conocimiento sobre modelado predictivo, sistemas ERP y CRM, así como el análisis de texto. También su formación en aprendizaje automático. 
7. La capacidad para implementar tecnologías de gestión de datos, reportería y elaboración de informes. 
8. Saber sobre Machine Learning, Deep Learning, Data wrangling, Data mining, Data Lake, Data Workflow, Procesos ETL, data munging o data tyding. 
9. Conocimiento y uso de herramientas de Big Data y explotación de datos.

## Responsabilidades y funciones

1. Desarrollo de estrategia de datos. 
2. Definir o mejorar Stack tecnológico para Big Data. 
3. Identificación de fuentes de datos. 
4. Diseño y gestión de la arquitectura de datos. 
5. Planificación de soluciones de Big Data. 
6. Gestión del flujo de datos. 
7. Garantizar la accesibilidad, precisión y seguridad de los datos. 
8. Realización de evaluación y auditorías de datos.
9. Analizar el impacto comercial que ciertas opciones técnicas pueden tener en los procesos comerciales  o de negocio. 
10. Proporcionar especificaciones según las cuales se define, gestiona y entrega la solución. 
11. Investigar continuamente tecnologías emergentes y propone cambios a la arquitectura existente para reducir deuda técnica y mejorar la solución. 
12. Asegurar el cumplimiento de requerimientos no funcionales y calidad del software: medir la performance de la aplicación y conducir pruebas en relación a la performance, seguridad, etc. 
13. Liderazgo técnico: Supervisar, liderar y/o guiar al o los equipos de desarrollo. Realizar coaching y mentoring sobre problemas técnicos, ayudando a la evolución profesional del equipo de programadores.
14. Estimar esfuerzo de desarrollo de componentes y tareas además de colaborar con la planificación de trabajo. 

## Tecnologías

La tecnología dependerá de los requisitos del puesto. La tecnología general relacionada es:

- Ingest & Collect: Logstash, Log4J, Fluentd, IOT, GCP (Cloud SDK, Cloud Dataproc), etc.
- Herramientas de automatización: Ansible, Control M, etc.
- Storage, Base de datos relacional (SQL): Oracle, MariaDB, MySQL, PostgreSQL, etc.
- Storage, Base de datos no relacional (NoSQL): MongoDB, Redis, Cassandra, HBase, etc.
- Storage, File Storage: HDFS, Amazon (S3, Glacier), GCP (Cloud Storage, Cloud SQL, BigQuery), etc.
- Store - Stream Storage, Event & messaging systems: Amazon (Kinesis, DynamoDB Stream, SQS, SNS), Apache Kafka Stream, RabbitMQ, etc.
- Process & Analyze, Big Data querying tools: GCP (Cloud Dataproc, BigQuery), Pig, Apache Hive, Impala, Apache Solr, Apache Spooq, Apache Zeppelin, Apache HBase, etc.
- Process & Analyze, Big Data Machine Learning toolkits: Mahout, SparkML, H2O, etc.
- Tecnología Cloud Computing: Google Cloud Platform, AWS, Asure, etc.
- Motores de BigData: Spark (Spark/Scala/HDFS/Hive, Python, SQL, Shell).
- Framework de programación distribuida de BigData: Hadoop (Hadoop, Spark, HDFS , Map Reduce, Yarn, PIG, HIVE, HBase, Mahout, Spark MLLib, Solar, Lucene, Zookeeper, Oozie ).
- ETL tools: Apache Flume, Apache Sqoop, Apache HBase, Apache Hive, Apache Oozie, Apache Phoenix, Apache Pig, Apache ZooKeeper.
- Consume: Amazon (QuickSight), Tableau, Looker, TIBC Jaspersoft, Microstrategy, Kibana, Flot, Apache Zeppelin, GCP (Data Studio), IPython, Power BI, Power Apps, IBM Cognos, etc.
- Security: Apache Knox, Apache Ranger, etc.

